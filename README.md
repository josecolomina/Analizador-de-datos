# Pipeline de PredicciÃ³n de Ventas E-commerce

> [!NOTE]
> ğŸ‡ºğŸ‡¸ **[Read in English](README_en.md)**

## DescripciÃ³n General
Este proyecto implementa un pipeline de datos "end-to-end" para la predicciÃ³n de ventas en comercio electrÃ³nico. Genera datos transaccionales sintÃ©ticos, los procesa en formato de series temporales, entrena un modelo de Machine Learning (Random Forest) y visualiza los resultados en un dashboard interactivo.

El objetivo es demostrar un flujo de trabajo moderno y ligero de ingenierÃ­a de datos y ciencia de datos utilizando Python.

## Arquitectura
El pipeline consta de cuatro etapas principales:

1.  **GeneraciÃ³n de Datos**: Crea datos sintÃ©ticos realistas para productos, clientes y transacciones de ventas utilizando `Faker`.
2.  **Ingesta**: Carga datos CSV crudos en una base de datos analÃ­tica **DuckDB**.
3.  **Procesamiento**: Agrega datos transaccionales en cifras de ventas diarias y realiza ingenierÃ­a de caracterÃ­sticas (lags, medias mÃ³viles).
4.  **Modelado**: Entrena un **RandomForestRegressor** (Scikit-learn) para predecir ventas futuras y genera un pronÃ³stico de 30 dÃ­as.
5.  **VisualizaciÃ³n**: Presenta datos histÃ³ricos y pronÃ³sticos a travÃ©s de un dashboard de **Streamlit**.

## Stack TecnolÃ³gico
-   **Lenguaje**: Python 3.10+
-   **Base de Datos**: DuckDB (Base de datos SQL OLAP en proceso)
-   **Procesamiento de Datos**: Pandas, NumPy
-   **Machine Learning**: Scikit-learn
-   **Dashboard**: Streamlit
-   **VisualizaciÃ³n**: Matplotlib

## Estructura del Proyecto
```
â”œâ”€â”€ data/               # Almacenamiento de datos (CSVs crudos y base de datos DuckDB)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ generator.py    # Generador de datos sintÃ©ticos
â”‚   â”œâ”€â”€ ingestion.py    # Carga de datos y creaciÃ³n de esquema
â”‚   â”œâ”€â”€ processing.py   # IngenierÃ­a de caracterÃ­sticas y agregaciÃ³n
â”‚   â”œâ”€â”€ model.py        # Entrenamiento e inferencia del modelo ML
â”‚   â””â”€â”€ dashboard.py    # AplicaciÃ³n Streamlit
â”œâ”€â”€ main.py             # Orquestador del pipeline
â”œâ”€â”€ requirements.txt    # Dependencias del proyecto
â””â”€â”€ README.md           # DocumentaciÃ³n del proyecto
```

## ConfiguraciÃ³n y Uso

### 1. InstalaciÃ³n
Clona el repositorio e instala las dependencias requeridas:

```bash
git clone <url-del-repositorio>
cd data_pipeline_project
pip install -r requirements.txt
```

### 2. Ejecutar el Pipeline
Ejecuta el pipeline de datos completo (generaciÃ³n -> ingesta -> procesamiento -> modelado):

```bash
python3 main.py
```

### 3. Lanzar el Dashboard
Inicia el dashboard interactivo para ver los resultados:

```bash
streamlit run src/dashboard.py
```

## CrÃ©ditos
Desarrollado por **Jose Colomina Alvarez**.
